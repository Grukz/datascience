{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook shows how to manage text encodings with Python. It is mostly based in the [Unicode HOWTO](http://docs.python.org/2/howto/unicode), from the Python documentation. I have decided to start with these notes because handling encodings seems a pretty difficult task if you do not understand how Unicode works and how the are different ways of encode the same string. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Characters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Characters are the smallest units of a text. Usually, we think about letters, but everything that can have its own glyph (i.e., drawing), such as a number, a math symbol or a greek letter, is a character. A character code is a list of numerical values for a certain list of characters. The most ancient character code is the well known ASCII code. ASCI defined numeric codes for 128 characters (from 0 to 127), mostly including English letters and numbers, but not (for example) accented characters. ASCII characters have an important feature: they use only 7 bits, so they can be always represented by a byte. In fact, another common encoding, Latin-1 (or ISO-8859-1), uses 8 bits, encoding 256 characters, using just a byte. The first 128 characters of Latin-1 are the same as characters of ASCII.  Python 2 strings are ASCII strings"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "print sys.getdefaultencoding()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ascii\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s='Hello, world!'\n",
      "print s\n",
      "# This only works because we are using Ipython notebooks, see later\n",
      "s='\u20ac'\n",
      "print s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hello, world!\n",
        "\u20ac\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "print sys.getdefaultencoding()\n",
      "print sys.stdin.encoding\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ascii\n",
        "None\n",
        "UTF-8\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Unicode"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You do not know yet what is an encoding, but remember this: Unicode is *not* an encoding. Unicode is a character code, just like ASCII. Unicode uses 16 bits for each character code (actually, modern versions include more), i.e. you have 2^16 = 65,536 distinct values available. Python includes a library, unicodedata, to display information about unicode characters, and their numeric codes (called code points, and usually represented using base 16). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import unicodedata\n",
      "u=unichr(8364)\n",
      "print u,ord(u),unicodedata.category(u),unicodedata.name(u)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u20ac 8364 Sc "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "EURO SIGN\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One important (and confusing...) characteristic of Unicode, is that the first 127 characters are the same than those of ASCII, and the first 256 are the same than those of Latin-1. To tell Python that you are specifiying a Unicode string, precede the string with a u, and specify the non-ascii character using its Unicode code points value, using 4 hex digits (you can specify it with 8 hex digits, using \\U):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u=u'This strings include an \\u20AC sign'\n",
      "print u"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "This strings include an \u20ac sign\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Encoding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is necessary, for example, when you want to write your text to an IO device, or display it in your system terminal, you need to represent a character numercial code usign bytes. This is trivial for ASCII or Latin-1: just represent the character with the numerical code, using the byte's 8 bits. But, what should we do with characters whose code goes over 256? You have no option: use more than one byte. The different forms to represent Unicode characters using bytes are called _encodings_\n",
      "\n",
      "The first think we must understand that every document (including Python sources) is encoded using a certain encoding. _You cannot surely know the encoding inspecting the file contents_, simple because the encoding is a way to transform bytes to characteres... the same file will look different using different encodings. However, there are some commands (in linux, for example the enca command) that tries to guess it (using \"a mixture of parsing, statistical analysis, guessing and black magic to determine their encodings\"). So, generally speaking, you should better know which encoding the file you want to read uses.\n",
      "\n",
      "For example, if this notebook shows correctly the \u20ac sign when we print a variable value, your default system Locale is using UTF-8. Let's verify it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import locale\n",
      "print locale.getpreferredencoding()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "UTF-8\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In Python, the default encoding for your code is ascii. If you do not specify another encoding, this Python code will not execute:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "s='\u20ac'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "yielding the following error"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "SyntaxError: Non-ASCII character '\\xe2' in file prueba_encoding.py on line 1, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To tell Python that you are using a different encoding (say, utf-8), the first or second line of your source must declare it (that is what the PEP referenced by the error message specifies)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# coding:utf-8\n",
      "s='\u20ac'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember, declaring the encoding does not means that your source it is actually encoded by your OS using the referenced encoding. You sould save your file in the correct format, using (for example) your text editor."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Unicode revisited"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The recommendation for handling character sequences in Python is to always use Unicode string for processing, to fix a standard code (that is, after all, what Unicode was invented for). To convert a string to Unicode, simply precede it with an 'u'. Python will use you declared encoding to interpret it (or you can specify special characters using \\u for unicode codes, \\x for hexa values, and so on and so forth):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s=u'abcdef\\u20AC'\n",
      "print s, type(s)\n",
      "s=u'abcdef\u20ac'\n",
      "print s, type(s)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "abcdef\u20ac <type 'unicode'>\n",
        "abcdef\u20ac <type 'unicode'>\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The unicode constructor allows to build a unicode type from a string. If you specify no encoding, the constructor will asume you are passing an  ascii-encoded string (and it will fail miserably trying to convert a character whose code is beyond 127)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u=unicode('This strings is ascii')\n",
      "print u, type(u)\n",
      "u=unicode('This string contains \\xE2\\x82\\xAC', encoding='utf-8')\n",
      "print u,type(u)\n",
      "u=unicode('This string contains \\xE2\\x82\\xAC')\n",
      "print u,type(u)\n",
      "u=unicode('This string contains \u20ac')\n",
      "print u,type(u)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnicodeDecodeError",
       "evalue": "'ascii' codec can't decode byte 0xe2 in position 21: ordinal not in range(128)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-49-67d9fc9c72fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This string contains \\xE2\\x82\\xAC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This string contains \\xE2\\x82\\xAC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This string contains \u20ac'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 21: ordinal not in range(128)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "This strings is ascii <type 'unicode'>\n",
        "This string contains \u20ac <type 'unicode'>\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a very common error. You should really be sure that you understand what is happening. In the first example, we simply pass a 8-bit string, and Python creates a unicode value, assuming ascii. In the second, we specify the Euro sign using hexa values, and tell Python that the 8-bit ascii string we passed is actually encoding sometingh using utf-8. In the third case, Python tries to create the Unicode character, but when he comes to character '\\xe2' (that is, a decimal value of 226), discover that it is an invalid ascii value, and fails. This is *completely independent* if the encoding you are using for your source:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u=unicode('This string contains \u20ac')\n",
      "print u,type(u)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnicodeDecodeError",
       "evalue": "'ascii' codec can't decode byte 0xe2 in position 21: ordinal not in range(128)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-50-b301343705cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This string contains \u20ac'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 21: ordinal not in range(128)"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Most string functions still work if you pass Unicode sequences instead of strings. The advantage is that, once you encoded the Unicode sequence, you forget about bytes and encodings. If you ask for the length of a sequence, you are asking for the number of Unicode code values, which is encoding-indepenteng (something that does not happen with bytes, since, for characters over 256, you need more than one byte to encode them). So, the general advice is: convert your string to Unicode before working with them. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Input/Output"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Most problems with encodings arise when you do input/output. The purpose of this section is to explain what happens when you read/write strings from a file, and how to avoid strange encoding errors. Let me cite the Unicode HOWTO:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_Unicode data is usually converted to a particular encoding before it gets written to disk or sent over a socket. It\u2019s possible to do all the work yourself: open a file, read an 8-bit string from it, and convert the string with unicode(str, encoding). However, the manual approach is not recommended._\n",
      "\n",
      "_One problem is the multi-byte nature of encodings; one Unicode character can be represented by several bytes. If you want to read the file in arbitrary-sized chunks (say, 1K or 4K), you need to write error-handling code to catch the case where only part of the bytes encoding a single Unicode character are read at the end of a chunk._"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_To read bytes from a file and convert them to Unicode characters, the **codecs** module implements an open() function  that returns a file-like object that assumes the file\u2019s contents are in a specified encoding and accepts Unicode parameters for methods such as .read() and .write()_"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I've included some files to test our work. Let's start by reading an ascii file, using the traditional file.open() function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ascii_file =open('../data/ASCII_file.txt')\n",
      "for line in ascii_file:\n",
      "        print line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "This is a plain ascii file. It does not matter which encoding it uses.\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, try to open the utf-8 file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "utf_8_file =open('../data/utf-8_file.txt')\n",
      "for line in utf_8_file:\n",
      "        print line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "This is an UTF-8. It includes a pair of japanese characters: \u6771\u4eac (Tokyo). The euro sign is this: \u20ac. Some accented characters: c\u00e1mara alegr\u00eda ilusi\u00f3n.\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First observation. You _can_ use open on encoded files. Open just considers them a stream of bytes. If you have the correct encoding in your terminal, the string will be displayed correctly. But beware that you are working with bytes, not with Unicode. That is not what is recommended, for the reasons previously mentioned. Let's see what happens if we try to open/display the Latin 1 file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "latin_1_file =open('../data/latin_1_file.txt')\n",
      "for line in latin_1_file:\n",
      "        print line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "This is a latin-1 encoded file. It includes some accented characters: educaci\ufffdn alegr\ufffda c\ufffdmara. \n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not working. Actually, it _is_ working: it just reads bytes, and display them... using utf-8, not latin-1. We can try to reencode each line, specifying the encoding (still not using Unicode!):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "latin_1_file =open('../data/latin_1_file.txt')\n",
      "for line in latin_1_file:\n",
      "        print line.decode('latin-1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "This is a latin-1 encoded file. It includes some accented characters: educaci\u00f3n alegr\u00ed\u00ada c\u00e1mara. \n"
       ]
      }
     ],
     "prompt_number": 63
    }
   ],
   "metadata": {}
  }
 ]
}