{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices for data scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guillermo Moncecchi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanto to do machine learning, you need to work with matrices with the same fluency you have with numbers.This notebooks aims to review the main properties of matrices, and inspect how to manipulate them using  `numpy` and `scipy` (see [the difference](http://www.scipy.org/scipylib/faq.html#what-is-the-difference-between-numpy-and-scipy)). It is not a numpy/scipy tutorial, but a brief introduction to matrices, showing also how to manipulate them using Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Very basic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create some numpy arrays, the common structure for dealing with n-dimensional matrices. I will mostly work with two-dimensional matrices since are easier to interpret, but most results admints n-dimensional  matrices (I guess...). The reference I used here were the first chapters of the [notes](http://imerl.fing.edu.uy/gal1/Curso_2011/main.pdf) (pdf) for the \"Geometría y Álgebra Lineal\" from Universidad de la República, Uruguay, a course I took many many years ago. For a  tutorial on `numpy`, refer to [this](http://www.engr.ucsb.edu/~shell/che210d/numpy.pdf) one, by M. Scott Shell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a 2D-numpy array, and show that its elements are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [1 1 0]\n",
      " [1 1 1]]\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# Specify each row of the matrix as a Python list\n",
    "a=np.array([[0,0,1],[1,1,0],[1,1,1]])\n",
    "print a\n",
    "print a.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an matrix of 3 rows by 4 columns, filled with zeros and another filled with ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print np.zeros((3,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print np.ones((3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D-arrays in Python are always row vectors, i.e. vectors are just $1\\times n$ matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print np.ones(10)\n",
    "print np.ones(10).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First operation: *matrix sum* (both matrices should have the same dimensionns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 2]\n",
      " [1 0 2]\n",
      " [1 2 2]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([0,1,1,0,0,1,1,1,1]).reshape((3,3))\n",
    "b=np.array([1,1,1,1,0,1,0,1,1]).reshape((3,3))\n",
    "print a + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the two matrixs have different dimensions, numpy _upcasts_ matrixes (adding elements to make both matrixes compatibles). In the following example the only row in b is repeated to make b a 2x2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 1]]\n",
      "[0 1]\n",
      "[[0 2]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([0,1,2,1]).reshape((2,2))\n",
    "b=np.array([0,1])\n",
    "\n",
    "print a\n",
    "print b\n",
    "print a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second operation: multiply a scalar by a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.1  8.1  8.1]\n",
      " [ 8.1  8.1  8.1]]\n"
     ]
    }
   ],
   "source": [
    "a=8.1\n",
    "b=np.ones((2,3))\n",
    "print a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third operation: Matrix product. Given $A(n,p)$ and $B(p,m)$, $A.B=C$, where $C=((c_{ij}))$ and $c_{ij}=\\Sigma_{h=1}^p a_{ih}b_{hj}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 1]\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [0 0 0]]\n",
      "[[2 1]\n",
      " [1 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([1,2,1,0,1,1,0,1,1,0,0,0]).reshape(4,3)\n",
    "print A\n",
    "B=np.array([[2,1],[1,1],[0,0]])\n",
    "print B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 3]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "print A.dot(B) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 3]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "print np.dot(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find easier to see matrix product as a matrix where each row in the product is the dot product of the corresponding row in A ($A_i$) and B. numpy allows to slice columns and rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A: (4, 3)\n",
      "[4 3]\n",
      "[1 1]\n",
      "[1 1]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "print \"Shape of A:\",A.shape\n",
    "for i in range(A.shape[0]):\n",
    "    print np.dot(A[i],B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can see the product matrix as a matrix where each column is the dot product of A and the corresponding column in B ($B^j$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 1 0]\n",
      "[3 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "for j in range(B.shape[1]):\n",
    "    print np.dot(A,B[:,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that resulting arrays are row arrays, not (as you could have expected) column arrays. The reason is simple: 1-D arrays in numpy are all the same (no matter if they row or column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Matrix transposition*: change columns into rows, and viceversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 1]\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [0 0 0]]\n",
      "[[1 0 0 0]\n",
      " [2 1 1 0]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print A\n",
    "print np.transpose(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting property (prove it!): \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(A\\cdot B)^t = B^t \\cdot A^t$ **[1]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 1 1 0]\n",
      " [3 1 1 0]]\n",
      "[[4 1 1 0]\n",
      " [3 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print np.transpose(A.dot(B))\n",
    "print np.transpose(B).dot(np.transpose(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Matrices as linear transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Wikipedia](http://en.wikipedia.org/wiki/Matrix_%28mathematics%29): \n",
    "\n",
    "_A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three dimensional space is a linear transformation which can be represented by a rotation matrix R. If v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two matrices is a matrix that represents the composition of two linear transformations.\n",
    "\n",
    "The following picture shows how the unit square is transformed under a linear transformation represented by a 2-by-2 matrix. I found that this use of matrices can let us imagine better the abstract properties we will describe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://upload.wikimedia.org/wikipedia/commons/a/ad/Area_parallellogram_as_determinant.svg\" width=\"350\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='http://upload.wikimedia.org/wikipedia/commons/a/ad/Area_parallellogram_as_determinant.svg',width=350)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors represented by a 2-by-2 matrix correspond to the sides of a unit square transformed into a parallelogram.\n",
    "Let's show this in Python. Suppose we have the matrix [[1,1],[2,1]]. This matrix represents a rotation, where each of point in $R^2$ (x,y) is mapped to (x+2y,x+y). Let's verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [2 1]]\n",
      "[3 2]\n"
     ]
    }
   ],
   "source": [
    "Ex=np.array([[1,1],[2,1]])\n",
    "print Ex\n",
    "\n",
    "print np.dot([1,1],Ex) # [1+2,1+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the unit square is transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [2 1]\n",
      " [1 1]\n",
      " [3 2]]\n"
     ]
    }
   ],
   "source": [
    "us=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "print np.dot(us,Ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inverse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $(n,n)$ matrix A is _invertible_ if there is a matrix B such that $A \\cdot B = I$, where $I$ is the identity matrix. B is the _inverse matrix_ of $A$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -1.  0.]\n",
      " [-1.  0.  1.]\n",
      " [ 1.  1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([1,1,1,0,1,1,1,2,1]).reshape((3,3))\n",
    "B=np.linalg.inv(A)\n",
    "print B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print np.dot(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print np.dot(B,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the inverse of some matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.75  0.5 ]\n",
      " [ 0.25  0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([2,-2,-1,3]).reshape((2,2))\n",
    "print np.linalg.inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear transformations (you guessed it), inverse matrices represent the linear transformation that gives back the original plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Basic Matrix Identities (from Bishop's book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notes for this section (and for the following ones) try to create an numpy version of the notes in appendix C of the _\"Pattern Recognition and Machine Learning\"_ book by Bishop, called \"Properties of Matrices\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [0 1 1]\n",
      " [1 2 1]]\n",
      "[[1 0 1]\n",
      " [0 1 1]\n",
      " [0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([1,1,1,0,1,1,1,2,1]).reshape((3,3))\n",
    "B=np.array([1,0,1,0,1,1,0,1,2]).reshape((3,3))\n",
    "print A\n",
    "print B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Property 1:*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A\\cdot A^{-1} = A^{-1}\\cdot A = I$ **[2]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, _if we multiply a matrix by its inverse, we get the identity matrix (no matter the product order)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print np.dot(A,np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print np.dot(np.linalg.inv(A),A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Property 2_: $(AB)^{-1} = B^{-1}A^{-1}$ **[3]**. Proof: just left multiply with $AB$ both sides and get $I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -2.  2.]\n",
      " [-3. -1.  3.]\n",
      " [ 2.  1. -2.]]\n",
      "[[-1. -2.  2.]\n",
      " [-3. -1.  3.]\n",
      " [ 2.  1. -2.]]\n"
     ]
    }
   ],
   "source": [
    "print np.linalg.inv(np.dot(A,B))\n",
    "print  np.dot(np.linalg.inv(B),np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Property 3_: $(A^T)^{-1}= (A^{-1})^T$ **[4]**. _The inverse of the trasposed is the trasposed inverted_ \n",
    "\n",
    "Proof: using Property 1 and transposing, we get $(AA^{-1})^{T}=I$. Applying [1], we get $(A^{-1})^T A^T=I$. Multiplying both sides to the right by  $(A^T)^{-1}$, we get the result we were searching for. Let's verify it using our examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -1.  1.]\n",
      " [-1.  0.  1.]\n",
      " [-0.  1. -1.]]\n",
      "[[ 1. -1.  1.]\n",
      " [-1.  0.  1.]\n",
      " [ 0.  1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "print np.linalg.inv(np.transpose(A))\n",
    "print np.transpose(np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some definitions. A set of vectors is _linearly independent_ if none of the vectors can be expressed as a lineal combination of the remainder (or, equivalently, if $\\sum_{n} \\alpha_{n}a_n = 0$ only if all $\\alpha_{n}$). The _rank_ of a matrix is the maximum number of linearly independent rows (or, equivalently, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Square Matrixs. Traces and determinants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _trace_ of a matrix is defined as the sum of the elements of its diagonal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [0 1 1]\n",
      " [1 2 1]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print A\n",
    "print np.trace(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $Tr(AB)=Tr(BA)$ **[5]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 = 8\n"
     ]
    }
   ],
   "source": [
    "print np.trace(np.dot(A,B)),\"=\",np.trace(np.dot(B,A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant $|A|$ of a $N\\times N$ matrix is defined by:\n",
    "\n",
    "$|A| = \\sum (\\pm 1)A_{1i_1}A_{2i_2}\\ldots A_{Ni_N}$, where \"the sum is taken over all products consisting of precisely one element from each row and one element from each column, with a coefficient $+1$ or $-1$ according to whether the permutation $i_1i_2\\ldots i_N$ is even or odd, respectively\".  From Wikipedia: _\"A geometric interpretation can be given to the value of the determinant of a square matrix with real entries: the absolute value of the determinant gives the scale factor by which area or volume (or a higher-dimensional analogue) is multiplied under the associated linear transformation, while its sign indicates whether the transformation preserves orientation. Thus a 2 × 2 matrix with determinant −2, when applied to a region of the plane with finite area, will transform that region into one with twice the area, while reversing its orientation\"_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Property: $|AB| = |A||B|$ **[6]** (The determinant of a product of matrices is the product of their determinants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "-1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print np.linalg.det(np.dot(A,B))\n",
    "print np.linalg.det(A), np.linalg.det(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinant of the inverse matrix: $|A^{-1}| = \\frac{1}{|A|}$  **[7]**(The determinant of the inverse of a matrix is the real inverse of its determinant). Note that this implies that a matrix is invertible if it has a non-zero determinant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "C=np.array([2,-2,-1,3]).reshape((2,2))\n",
    "print np.linalg.det(np.linalg.inv(C))\n",
    "print np.linalg.det(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If A and B are of size $N \\times M$, then $|I_N + AB^T| = |I_M + A^TB|$ **[7]** A special case is: $|I_N + ab^T| = |1 + a^Tb|$, where a and b are N-dimensional column vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  2.  1.  2.]\n",
      " [ 2.  5.  2.  4.]\n",
      " [ 3.  6.  4.  6.]\n",
      " [ 4.  8.  4.  9.]]\n",
      "17.0\n",
      "17.0\n"
     ]
    }
   ],
   "source": [
    "a=np.array([1,2,3,4]).reshape(4,1)\n",
    "b=np.array([1,2,1,2]).reshape(4,1)\n",
    "print np.eye(4) + np.dot(a,np.transpose(b))\n",
    "print np.linalg.det(np.eye(4)+np.dot(a,np.transpose(b)))\n",
    "print 1.0 + np.dot(np.transpose(a).reshape(4),b.reshape(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Eigenvector Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If A is of size $M \\times M$, then the eigenvector equation is $Au_i = \\lambda _i u_i$ ($u_i$ is an _eigenvector_ and $\\lambda_i$ is its corresponding _eigenvalue_). If you have a vector $v$, and a matrix $A$, the dot product $Av$ produces another vector $v'$. If $v$ and $v'$ are parallel, then $v$ is an eigenvector of $A$, and the scale factor between $v$ and $v'$ is its eigenvalue. In the following picture (taken from the beautiful Wikipedia page for Eigenvalues and Eigenvectors), the transformation produced by the matrix [[2,1],[1,2]] is shown; blue and pink vectors are eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://upload.wikimedia.org/wikipedia/commons/a/ad/Eigenvectors-extended.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='http://upload.wikimedia.org/wikipedia/commons/a/ad/Eigenvectors-extended.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalue equation for $A$ is $(A -\\lambda I)v =0$. This equation has a non-zero solution iif its determinant $|A - \\lambda I|=0$; its roots are precisely the eigenvalues of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the following A (Wikipedia, again!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0]\n",
      " [0 3 4]\n",
      " [0 4 9]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([2,0,0,0,3,4,0,4,9]).reshape(3,3)\n",
    "print A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its rank is the number of nonzero eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print np.linalg.matrix_rank(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the eigenvalues and eigenvectors (note that any non-zero multiple of an eigenvector is also an eigenvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 11.   1.   2.]\n",
      "[[ 0.          0.          1.        ]\n",
      " [ 0.4472136   0.89442719  0.        ]\n",
      " [ 0.89442719 -0.4472136   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print np.linalg.eig(A)[0] #Eigenvalues\n",
    "print np.linalg.eig(A)[1] #Eigenvectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Symmetric Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrices have the property that $A^T=A$. The inverse of a matrix is also symmetric (demonstrate). For symmetric matrices, the eigenvalues are real. See, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1 1]\n",
      " [1 5 0]\n",
      " [1 0 1]]\n",
      "[ 0.55051026  3.          5.44948974]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([3,1,1,1,5,0,1,0,1]).reshape(3,3)\n",
    "print A\n",
    "print np.linalg.eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors of a matrix can be choosen to be orthonormal (i.e. $u_i^Tu_j = I_{ij}$, and, by normalizing, can be set to unit length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.66533453694e-16\n",
      "-1.38777878078e-16\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "ev=np.linalg.eig(A)[1]\n",
    "print np.dot(ev[0],ev[1])\n",
    "print np.dot(ev[1],ev[2])\n",
    "print np.dot(ev[1],ev[1])\n",
    "print np.linalg.norm(ev[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any M-dimensional vector can be expressed as a linear combination of the eigenvectors.\n",
    "\n",
    "Let's define an $M \\times M$ matriz **U**, where the eigenvectors $u_i$ are the columns. This matrix satisfies $U^TU=I$. This matrix is _orthogonal_. The eigenvector equation can be defined now as: $AU = U\\Lambda$ **[8]**, where $\\Lambda$ is a diagonal matrix whose elements are the eigenvalues. It can be shown that multiplication by $U$ can be interpreted as a rigid rotation of the coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40824829 -0.81649658 -0.40824829]\n",
      " [-0.09175171  0.40824829 -0.90824829]\n",
      " [-0.90824829 -0.40824829 -0.09175171]]\n"
     ]
    }
   ],
   "source": [
    "# Show the eigenvectors of A\n",
    "print ev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22474487 -2.44948974 -2.22474487]\n",
      " [-0.05051026  1.22474487 -4.94948974]\n",
      " [-0.5        -1.22474487 -0.5       ]]\n",
      "[[ 0.22474487 -2.44948974 -2.22474487]\n",
      " [-0.05051026  1.22474487 -4.94948974]\n",
      " [-0.5        -1.22474487 -0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# Verify that AU = U\\Lambda\n",
    "print np.dot(A,ev)\n",
    "\n",
    "lmbda= np.diag(np.linalg.eigvals(A))\n",
    "print np.dot(ev,lmbda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From $AU = U\\Lambda$, it follows $U^TAU = \\Lambda$  ($A$ is _diagonalized_ by $U$). From this, left multiplying by $U$, right multiplying by $U^T$, we get \n",
    "\n",
    "$A = U \\Lambda U^T$, or $A = \\sum_{i=1}^M {\\lambda_i} u_i u_i^T$ **[9]**\n",
    "\n",
    "Taking inverse and because $U^{-1} = U^T$, we have: \n",
    "\n",
    "$A^{-1} = U \\Lambda^{-1} U^T$, or $A^{-1} = \\sum_{i=1}^M \\frac{1}{\\lambda_i} u_i u_i^T$ **[10]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.00000000e+00   1.00000000e+00   1.00000000e+00]\n",
      " [  1.00000000e+00   5.00000000e+00  -1.02001740e-15]\n",
      " [  1.00000000e+00  -8.88178420e-16   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# This should return A\n",
    "print ev.dot(lmbda.dot(np.transpose(ev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55555556 -0.11111111 -0.55555556]\n",
      " [-0.11111111  0.22222222  0.11111111]\n",
      " [-0.55555556  0.11111111  1.55555556]]\n",
      "[[ 0.55555556 -0.11111111 -0.55555556]\n",
      " [-0.11111111  0.22222222  0.11111111]\n",
      " [-0.55555556  0.11111111  1.55555556]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the inverse of A\n",
    "print ev.dot(np.linalg.inv(lmbda).dot(np.transpose(ev)))\n",
    "\n",
    "print np.linalg.inv(A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also show that the determinant of a matrix is the products of its eigenvalues:\n",
    "\n",
    "$|A| = \\prod_{i=1}^{M}\\lambda_i$ **[11]**\n",
    "\n",
    "And that the trace of a matrix is the sum of its eigenvalues\n",
    "\n",
    "$Tr(A) = \\sum_{i=1}^{M}\\lambda_i$ **[12]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0 9.0\n",
      "9.0 9\n"
     ]
    }
   ],
   "source": [
    "print np.prod(np.linalg.eigvals(A)), np.linalg.det(A)\n",
    "\n",
    "print np.sum(np.linalg.eigvals(A)), np.trace(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix A is _positive (semi)definite_ if, for all w, $w^T A w >(\\geq) 0$. A positive definite matrix has all positive eigenvalues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Matrix Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a function $f$ mapping from $m$-by-$n$ matrices to real numbers, we can define the _gradient_ $\\nabla_A f(A)$ as an $m$-by-$n$ matrix whose $(i,j)$ element is $\\partial f/\\partial A_{ij}$. Some properties of gradients (see Bishop's appendix for details), are the following:\n",
    "\n",
    "- $\\frac{\\partial}{\\partial A}Tr(AB)=B^T$ **[11]**\n",
    "- $\\frac{\\partial}{\\partial A}Tr(A^TB)=B$ **[12]**\n",
    "- $\\frac{\\partial}{\\partial A}Tr(A)=I$ **[13]**\n",
    "- $\\frac{\\partial}{\\partial A}Tr(ABA^T)=A(B+B^T)$**[14]**\n",
    "\n",
    "And the beautiful\n",
    "\n",
    "- $\\frac{\\partial}{\\partial A}\\ln|A|=(A^{-1})^T$**[15]**\n",
    "\n",
    "Yet another two properties of gradients (from Andrew Ng's course notes, see next section):\n",
    "\n",
    "- $\\frac{\\partial}{\\partial{A^T}} f(A)= (\\frac{\\partial}{\\partial A} f(A))^T$ **[16]**\n",
    "- $\\frac{\\partial}{\\partial{A}} Tr(ABA^TC) = CAB + C^TAB^T$ **[17]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Matrices for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to find a machine learning method that do _not_ use matrices. From linear regression to deep neural networks, going through Principal Component Analysis, things are much easier if we use matrices and vectors. Just as an example, let's show how find a closed-form solution for the problem of linear regression, following exactly the magnificient [notes](http://cs229.stanford.edu/notes/cs229-notes1.pdf) of Mr. @andrewyng. In the next section, we will see how to implement everything using Python.\n",
    "\n",
    "First, let's define the problem of linear regression: we have a number of input _features_ from $\\mathbb{R}^n$, noted as $x$, and an associated _target value_ $y$ in $\\mathbb{R}$. Our _hypothesis_ is that, given $x$, $h(x) = \\sum_{i=0}^{i=n} \\theta_i x_i$ (assuming that $x_0=1$), or, using matrix notation (see how easier it looks):\n",
    "\n",
    "$h(x)=\\theta^T x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that, by convention, and instance from $\\mathbb{R}^n$ is considered a column vector). What we want to do is to find $\\theta^T$, given a training set (i.e. instances from $\\mathbb{R}^n$ and their associated target value). The cost function we want to minimize wrt $\\theta$ is the following:\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{i=m} (h_\\theta(x^{(i)})- y^{(i)})^2$\n",
    "\n",
    "where $x^{(i)}$ denotes the i-th instance, and $y^{(i)}$ its value. Note we are squaring the difference to take into account only the absolute value of the differences. \n",
    "\n",
    "This method is called _ordinary least squares_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical way for minimizing $J$ is using some numerical method, i.e. _gradient descent_, which allows to minimize any convex function of multiple variables. For ordinary least squares, however, there is a _closed form_ solution, which we will show. The use of matrix notation will make things much easier than if we had to use index notation. \n",
    "\n",
    "First, let's rewrite J using matrix notation.\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2}(X\\theta - y)(X\\theta - y)$\n",
    "\n",
    "where $X$ is the _design matrix_ that includes the training instances input values as rows, and $y$ is a column matrix including the corresponding target values (you can use index notation to check the equivalence, keeping in mind that $zz^T$ equals the sum of the squared elements of $z$!)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, things are easy (especially if you compare to using index notation). First, use [1]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_\\theta J(\\theta) =\\nabla_\\theta\\frac{1}{2}(X\\theta - y)^T(X\\theta - y) = \\nabla_\\theta\\frac{1}{2} (\\theta^TX^TX\\theta - \\theta^TX^Ty - y^TX\\theta+ yy^T) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since $J(\\theta)$ is a real number, its trace is the same number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_\\theta J(\\theta) =\\nabla_\\theta\\mathrm{Tr}(\\frac{1}{2} (\\theta^TX^TX\\theta - \\theta^TX^Ty - y^TX\\theta+ yy^T))\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, since $\\mathrm{Tr}(A) = \\mathrm{Tr}(A^T)$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{1}{2} \\nabla_\\theta(\\mathrm{Tr}(\\theta^TX^TX\\theta) - 2 \\mathrm{Tr}(\\theta^TX^Ty) + \\mathrm{Tr}(yy^T))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Applying [16] and [17] we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{1}{2} (X^TX\\theta + X^TX\\theta -2X^Ty )= X^TX\\theta -X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize $J$, we just equals its gradient to 0, obtaining,\n",
    "\n",
    "$$\n",
    "X^TX\\theta = X^Ty\n",
    "$$\n",
    "\n",
    "and, from here, we get a closed formula for our parameter vector $\\theta$:\n",
    "\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Linear regression with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have target class _y_ (say, a house's price), dependent on 3 real-valued attributes $x_1,x_2,x_3$ (for example, number of rooms, minimum tax and house age). Suppose that we have the attributes and prices for 10 houses.  _If we assume_ that the price of a house is a _linear combination_ of the three attributes,  $y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3$, and that we should find the $\\theta$ values. Now, we can define $X$ (the design matrix) and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   1.   1.9  1.1]\n",
      " [ 1.   0.1  1.   1. ]\n",
      " [ 1.   1.1  1.   0.1]\n",
      " [ 1.   1.1  1.1  0.9]\n",
      " [ 1.   2.1  1.2  1. ]\n",
      " [ 1.   0.9  1.   3. ]\n",
      " [ 1.   3.1  0.9  3. ]\n",
      " [ 1.   1.9  0.9  2.1]\n",
      " [ 1.   0.  -0.1  1.1]\n",
      " [ 1.   1.   1.   0.1]]\n"
     ]
    }
   ],
   "source": [
    "X=np.array([\n",
    "[1.0,1.0,1.9,1.1],\n",
    "[1.0,0.1,1.0,1.0],\n",
    "[1.0,1.1,1.0,0.1],\n",
    "[1.0,1.1,1.1,0.9],\n",
    "[1.0,2.1,1.2,1.0],\n",
    "[1.0,0.9,1.0,3.0],\n",
    "[1.0,3.1,0.9,3.0],\n",
    "[1.0,1.9,0.9,2.1],\n",
    "[1.0,0.0,-0.1,1.1],\n",
    "[1.0,1.0,1.0,0.1]\n",
    "])\n",
    "print X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.   2.   4.1  3.   5.   1.   2.8  2.6 -0.4  4.1]\n"
     ]
    }
   ],
   "source": [
    "y=np.array([5.0,2.0,4.1,3.0,5.0,1.0,2.8,2.6,-0.4,4.1])\n",
    "print y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we added a left column to the matrix, each cell valued with 1. This is to add a new $x_0$ attribute, as before. We can now calculate the parameters for the hyperplane that minimizes the sum of squares using the previous formula ($\\theta = (X^TX)^{-1}X^Ty$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.87782702  1.051817    2.18366554 -1.05476926]\n"
     ]
    }
   ],
   "source": [
    "theta=np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X),X)),np.transpose(X)),y)\n",
    "print theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that our just found parameter vector, dot the attributes of each instance, gives us (approximately) the corresponding $y$ cell. We can do this for every training examples, just calculating $X\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.91836235  2.111905    4.11301432  3.48756547  4.6522721   0.84382008\n",
      "  2.93945092  2.62656285 -0.50078572  4.00783262]\n"
     ]
    }
   ],
   "source": [
    "print np.dot(X,theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, a) ordinary least squares is a very simple method for regression and b) every statistical/machine learning package includes much more efficient ways of finding $\\theta$. For example, using the linalg library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.87782702  1.051817    2.18366554 -1.05476926]\n"
     ]
    }
   ],
   "source": [
    "print np.linalg.lstsq(X, y)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aforementioned [notes](http://cs229.stanford.edu/notes/cs229-notes1.pdf) for CS229 course by Andrew Ng are an excelente introduction to Supervised learning, and make intensive use of matrix notation. From the same course,  a [Linear Algebra Review](http://cs229.stanford.edu/section/cs229-linalg.pdf) covers mostly the same topics we have seen, with some more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
